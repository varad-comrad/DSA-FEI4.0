{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FBds0D828RCA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dAuFMybR_asj"
      },
      "outputs": [],
      "source": [
        "max_len = 10\n",
        "n_embed = 64\n",
        "vocab_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pLaBMm76CDbF"
      },
      "outputs": [],
      "source": [
        "class Embedding:\n",
        "    __slots__ = '_n_embed', '_vocab_size', '_proxy', '_vocab'\n",
        "    def __init__(self, *, n_embed, vocab_size):\n",
        "        self._n_embed = n_embed\n",
        "        self._vocab_size = vocab_size\n",
        "        self._proxy = {}\n",
        "\n",
        "    def encode(self, vocab):\n",
        "        self._vocab = vocab\n",
        "        if len(vocab) != self._vocab_size:\n",
        "            raise ValueError(f'Mismatching amount of tokens: expected {self._vocab_size}, found {len(vocab)}')\n",
        "        aux = np.random.randn(self._vocab_size, self._n_embed)\n",
        "        for i, element in enumerate(vocab):\n",
        "            self._proxy[element] = aux[i]\n",
        "        return self\n",
        "\n",
        "    # Dont really like this, but whatever\n",
        "    def _tokenize(self, sentence):\n",
        "        tokens = []\n",
        "        current_token = \"\"\n",
        "\n",
        "        for char in sentence:\n",
        "            current_token += char\n",
        "\n",
        "            for token in self._vocab:\n",
        "                if current_token.endswith(token):\n",
        "                    tokens.append(current_token[:-len(token)])\n",
        "                    tokens.append(token)\n",
        "                    current_token = \"\"\n",
        "\n",
        "        if current_token:\n",
        "            tokens.append(current_token)\n",
        "\n",
        "        return list(filter(None, tokens))\n",
        "\n",
        "    def __call__(self, item: str):\n",
        "        return (self._proxy[element] for element in self._tokenize(item))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "0UmQOK2XDry-"
      },
      "outputs": [],
      "source": [
        "embedding = Embedding(n_embed=n_embed, vocab_size=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUTCvVfXDyGr",
        "outputId": "08a6705e-c53b-4bf1-82ed-7a5da4f0d760"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.Embedding at 0x7b788a938440>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding.encode(list(elem for elem in r'ABCDEFGHIJKLMNOPQRSTUVWXYZÀÁÉÍÓÚabcdefghijklmnopqrstuvwxyzáàéíóú0123456789%!&()\"\\'/\\\\.,-=_+ :;{}[]<>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMEIpr0TF5iy",
        "outputId": "216ef80f-46c4-4b96-d5e7-9551897c07fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([-1.01954795e+00,  1.71984344e+00, -1.62456924e+00, -7.54220092e-01,\n",
              "        -5.78118374e-01,  7.59163688e-01, -1.28430170e+00, -2.15455925e-01,\n",
              "         1.54189251e+00,  1.64011301e+00,  1.67926912e-02,  4.54669136e-01,\n",
              "        -7.23376809e-01,  1.24843070e+00, -5.55384705e-01,  3.48785027e-01,\n",
              "        -5.96646043e-01, -9.25733339e-01, -1.31328725e-01,  4.47514207e-01,\n",
              "         1.57619787e-01, -1.71094674e+00,  9.31087134e-04, -2.06651634e+00,\n",
              "         2.21277483e+00, -4.22706068e-01,  1.80693008e-01,  1.10720845e+00,\n",
              "         4.08925129e-01,  1.99574140e-02,  4.54759715e-01, -8.41287058e-02,\n",
              "         1.38895836e+00, -8.37352886e-02,  1.09673196e+00,  5.69760700e-01,\n",
              "        -4.51186613e-01,  9.47642772e-01, -1.06016111e+00,  4.20843095e-01,\n",
              "        -4.66375584e-01,  9.35814971e-01, -1.65224759e-01, -1.22507835e+00,\n",
              "         6.97088728e-01,  3.60726686e-01, -1.74727581e+00,  1.67500620e+00,\n",
              "        -2.88599079e-01,  3.53110597e-01, -9.85757428e-01,  9.51532929e-01,\n",
              "        -1.27160245e+00,  8.59811747e-01,  3.40470424e-01,  3.54721451e-01,\n",
              "        -2.49751728e+00,  2.88120409e-01,  3.05026153e-01, -1.14477641e+00,\n",
              "        -7.69636358e-01,  4.92631248e-01,  7.35911935e-02, -5.97411884e-02]),\n",
              " array([-0.46263721,  0.54561871,  0.32670647, -1.53881994, -0.955399  ,\n",
              "         0.49844085, -0.08657149, -0.39912498, -0.53782833, -0.06691756,\n",
              "        -0.49705584, -0.0915117 , -0.36617038,  0.5963134 , -1.29328743,\n",
              "         0.01658028,  0.12588337, -2.42704298,  0.37766217, -0.37467116,\n",
              "        -0.81650342, -1.43621833,  0.29102459,  0.21128225, -0.70577457,\n",
              "        -0.07900918, -0.78197465,  0.87422956,  2.07452115,  0.40858301,\n",
              "         1.06730432,  0.5967065 , -1.2867612 ,  1.3958998 , -1.39737479,\n",
              "        -1.14952676,  0.35678122, -0.17879966,  0.98643088, -1.57437795,\n",
              "        -1.03016072,  1.7033936 , -0.7989486 ,  1.09419849,  1.80943053,\n",
              "        -0.49795133,  0.60087615,  1.05290155, -0.39319163,  0.48910392,\n",
              "        -0.93102268,  0.40539613, -2.12005066, -0.64141505,  0.24815132,\n",
              "         1.12896103,  0.9746935 ,  0.25951626, -0.49882052, -0.07879812,\n",
              "         0.26716597,  0.66143349,  1.47975322, -0.49234821])]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding(['c', 'a'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "UGGZc0Y-F9jg"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding:\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        position = np.arange(max_len).unsqueeze(1)\n",
        "        div_term = np.exp(np.arange(0, d_model, 2)\n",
        "                             * (-math.log(10000.0) / d_model))\n",
        "        pe = np.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "SHZNqtfyPhPQ"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    def __init__(self, *dim):\n",
        "        self._matrix = np.random.randn(*dim)\n",
        "        self._dim = dim\n",
        "\n",
        "    @property\n",
        "    def matrix(self):\n",
        "        return self._matrix\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return np.dot(self._matrix, x)\n",
        "    \n",
        "    __call__ = forward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        return np.maximum(x, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LayerNorm:\n",
        "    def __init__(self, dim):\n",
        "        self._gamma = np.random.randn(dim)\n",
        "        self._beta = np.random.randn(dim)\n",
        "        self._dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = np.mean(x, axis=0)\n",
        "        std = np.std(x, axis=0)\n",
        "        return (x - mean) / std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "yoJu3rM-Pjyx"
      },
      "outputs": [],
      "source": [
        "class SoftMax:\n",
        "\n",
        "    __slots__ = '_dim',\n",
        "    def __init__(self, *, dim=-1):\n",
        "        self._dim = dim\n",
        "    def __call__(self, x, dim=None):\n",
        "        if dim is None:\n",
        "            dim = self._dim\n",
        "        ex = np.exp(x-np.max(x))\n",
        "        return ex / ex.sum(axis=dim).reshape(dim,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "SbH_2rY9PlY_"
      },
      "outputs": [],
      "source": [
        "class Head:\n",
        "\n",
        "    __slots__ = '_n_dim', '_q', '_k', '_v', '_softmax'\n",
        "\n",
        "    def __init__(self, n_dim, head_size):\n",
        "        self._n_dim = n_dim\n",
        "        self._q = Linear(n_dim, head_size)\n",
        "        self._k = Linear(n_dim, head_size)\n",
        "        self._v = Linear(n_dim, head_size)\n",
        "        self._softmax = SoftMax()\n",
        "\n",
        "    def forward(self, x, y, z, mask=None):\n",
        "        q = self._q.matrix @ x\n",
        "        k = self._k.matrix @ y\n",
        "        v = self._v.matrix @ z\n",
        "        logits = np.dot(q, k.T)\n",
        "        logits = logits + mask if mask is not None else logits\n",
        "        depth = k.shape[-1]\n",
        "        logits /= np.sqrt(depth)\n",
        "        weigths = self._softmax(logits)\n",
        "        return np.dot(weigths, v)\n",
        "\n",
        "    __call__ = forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "jhAmYW3FZ4Cm"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention:\n",
        "    def __init__(self, n_heads, head_dim, n_embed) -> None:\n",
        "        self._n_heads = n_heads\n",
        "        self._head_dim = head_dim\n",
        "        self._n_embed = n_embed\n",
        "        self._heads = [Head(n_embed, head_dim) for _ in range(n_heads)]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return np.concatenate([head(x) for head in self._heads], axis=-1)\n",
        "    \n",
        "    __call__ = forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIO63ZiebVn9"
      },
      "outputs": [],
      "source": [
        "class Encoder:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBlock:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBlock:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTransformer:\n",
        "    def __init__(self, n_heads, head_dim, n_embed, vocab_size, max_len) -> None:\n",
        "        self._n_heads = n_heads\n",
        "        self._head_dim = head_dim\n",
        "        self._n_embed = n_embed\n",
        "        self._vocab_size = vocab_size\n",
        "        self._max_len = max_len\n",
        "        self._embedding = Embedding(n_embed=n_embed, vocab_size=vocab_size)\n",
        "        self._transformer = _SimpleTransformer(n_heads, head_dim, n_embed)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self._transformer(self._embedding[list(x)])\n",
        "\n",
        "    __call__ = forward"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
